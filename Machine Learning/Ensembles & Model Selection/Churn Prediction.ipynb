{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Description\n\nThe idea of this educational project is to predict the churn of telecom company's clients. This problem is extremely important in practice and ML-algorithms are implemented to solve it in real telecom companies since it makes sense that if a company knows that a client is going to stop using its services then it can offer some extra bonuses to keep this client."},{"metadata":{},"cell_type":"markdown","source":"**Contact me in telegram:** @lawrence_grig\n\n**Name on leaderboard:** Лаврентий_Григорян"},{"metadata":{},"cell_type":"markdown","source":"# Import of modules"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Necessary libraries (for data analysis)\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as ss\nimport statsmodels\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\n# Plotly for interactive visualization\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nfrom plotly.subplots import make_subplots\nimport plotly\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\ninit_notebook_mode(connected=True)\n\n# Some libraries and objects and functions for ML\nimport xgboost\nimport lightgbm\nfrom catboost import CatBoostClassifier, cv, Pool\nfrom sklearn.preprocessing import (RobustScaler, StandardScaler, MinMaxScaler,\n                                   OneHotEncoder, LabelEncoder, PolynomialFeatures)\nfrom sklearn.model_selection import (train_test_split, GridSearchCV, RandomizedSearchCV,\n                                     cross_validate, cross_val_score, cross_val_predict)\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n                              AdaBoostClassifier, ExtraTreesClassifier, StackingClassifier)\nfrom sklearn.metrics import (accuracy_score, f1_score, confusion_matrix,\n                             roc_auc_score, roc_curve)\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.base import BaseEstimator\nfrom sklearn.decomposition import PCA\n\n# Some display options\nfrom tqdm.notebook import tqdm\nnp.set_printoptions(suppress=True)\nsns.set(style='whitegrid')\nplt.rcParams['figure.figsize'] = 8, 4\n# %config InlineBackend.figure_format = 'svg'\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data overview and primary data analysis"},{"metadata":{},"cell_type":"markdown","source":"Firstly, let's read our datasets, concatenate them and mark training and testing samples with 1 and 0 respectively for convenience:"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Getting train and test samples\ntrain = pd.read_csv('/kaggle/input/advanced-dls-fall-2020/train.csv')\ntest = pd.read_csv('/kaggle/input/advanced-dls-fall-2020/test.csv')\n\n# Sample submission\nsub = pd.read_csv('../input/advanced-dls-fall-2020/submission.csv')\n\n# Making a copy of train\ndf = train.copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train, test and dataset sizes\ndf.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's take a look at our data and get some basic info about it:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting features separately\n\n# Numeric features\nnum_cols = [\n            'ClientPeriod',\n            'MonthlySpending',\n            'TotalSpent'\n]\n\n# Categorical features\ncat_cols = [\n            'Sex',\n            'IsSeniorCitizen',\n            'HasPartner',\n            'HasChild',\n            'HasPhoneService',\n            'HasMultiplePhoneNumbers',\n            'HasInternetService',\n            'HasOnlineSecurityService',\n            'HasOnlineBackup',\n            'HasDeviceProtection',\n            'HasTechSupportAccess',\n            'HasOnlineTV',\n            'HasMovieSubscription',\n            'HasContractPhone',\n            'IsBillingPaperless',\n            'PaymentMethod'\n]\n\n# All features\nfeature_cols = num_cols + cat_cols\n\n# Target variable\ntarget_col = 'Churn'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualizing missing values\nmsno.matrix(df, figsize=(14, 7))\nplt.title('Missing values visualization', size=15);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this step we can see that:\n\n1. The dataset contains 20 columns: 19 predictors and the target named \"**Churn**\"\n\n2. There are 7029 observations\n\n3. From the first view, the dataset contains **no missing values** which is the positive news\n\n4. Obviously, \"**TotalSpent**\" is not a categorical feature - its data type requires a change\n\n5. \"**IsSeniorCitizen**\" is already binary, let's convert it to \"category\" for convenient data analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"# IsSeniorCitizen\ndf['IsSeniorCitizen'] = df['IsSeniorCitizen'].map({1: 'Yes', 0: 'No'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking TotalSpent values\ndf['TotalSpent'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since we don't know yet what this missing values (in form of spaces) mean, let's substitute them with 0 for now:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replacing missing values and changing TotalSpent dtype\ndf['TotalSpent'] = df['TotalSpent'].str.replace(' ', '0').astype('float64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Imputing TotalSpent with KNN - not correct\n# imputer = KNNImputer()\n# df['TotalSpent'] = imputer.fit_transform(pd.get_dummies(df.drop(['sample'], axis=1)))[:, 2]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Number of NaNs: {df.isna().sum().sum()}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* So all the NaNs are imputed"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Changing dtypes of categorical features to pandas \"category\"\nfor cat in cat_cols:\n    df[cat] = df[cat].astype('category')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check the data for duplicated rows/columns:"},{"metadata":{"trusted":true},"cell_type":"code","source":"dup_cols = df.T.duplicated()\ndup_rows = df.duplicated()\nprint(f'Number of duplicated columns: {dup_cols.sum()}\\nNumber of duplicated rows: {dup_rows.sum()}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Duplicated rows\ndf[df.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* There are 14 duplicates\n\n* We will decide later whether to delete them or not - this can be just the nature of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Dropping duplicates - reduced the model quality\n# df.drop_duplicates(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Descriptive statistics:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Numeric features\ndf.describe().iloc[:, :-1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Categorical features\ndf.describe(include=['category'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> No super important conclusions for descriptive statistics."},{"metadata":{},"cell_type":"markdown","source":"Let's check our target variable and see if we deal with disbalanced class problem:"},{"metadata":{"trusted":true},"cell_type":"code","source":"trace_0 = go.Bar(x=df['Churn'].value_counts().index,\n                 y=df['Churn'].value_counts().values,\n                 marker_color=['green', 'crimson'])\n\ndata = [trace_0]\nlayout = {'title': 'Churn bar plot',\n          'xaxis': {'title': 'churn category'},\n          'yaxis': {'title': 'number of observations'}}\n\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(height=600, width=800)\niplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* As it was expected, there is a class disbalance in favor of non-churn clients which signals that such metrics as **accuracy** will be less efficient in future ML-model evaluation\n\n* Probably we'll handle this problem a little bit later during ML-modelling."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Exploratory Data Analysis\n\nNow it makes sense to go over each feature and preprocess it separately."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training set\ndf.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the purposes of EDA let's build some basic functions for convenience:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_unique(data, column: str):\n    \"\"\"\n    Shows number of unique values\n    for given data and column\n    \n    + value counts\n    \"\"\"\n    print(f'Number of unique values for \"{column}\": {data[column].nunique()}\\n')\n    print(f'Value counts for {column}:', data[column].value_counts(), sep='\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def boxplot(data, column: str):\n    \"\"\"\n    Plots interactive boxplot \n    for given numeric feature\n    \"\"\"\n    trace = go.Box(y=data[column],\n                   x=data['Churn'],\n                   name=f'{column}')\n    traces = [trace]\n    layout = {'title': f'Boxplot for \"{column}\" depending on Churn'}\n    \n    fig = go.Figure(data=traces, layout=layout)\n    fig.update_traces(marker_color='rgb(158,202,225)',\n                      marker_line_color='rgb(8,48,107)',\n                      marker_line_width=1.5, opacity=1)\n    fig.update_layout(autosize=False,\n                      width=800,\n                      height=500)\n    iplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def distplot(data, column: str):\n    \"\"\"\n    Plots interactive distplot \n    for given numeric feature\n    \"\"\"\n    fig = ff.create_distplot([data[column].values],\n                             group_labels=[column],\n                             colors=['rgb(0, 200, 200)'])\n    fig.update_layout(title_text=f'Distplot for \"{column}\"',\n                      autosize=False,\n                      width=800,\n                      height=500)\n    fig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def norm_dist(data, column: str):\n    \"\"\"\n    Plots logarithmic, box-cox \n    transformed and original\n    data to check the presence of \n    normal distribution\n    \"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n    \n    # Original distribution\n    sns.distplot(data[column], ax=axes[0])\n    axes[0].set_title('Original distribution')\n    \n    # Log-normal distribution check\n    sns.distplot(data[column].apply(lambda x: np.log(x + 1)), ax=axes[1],\n                 color='green')\n    axes[1].set_title('Log-normal distribution');\n    \n    # Distribution after box-cox\n    try:\n        sns.distplot(ss.boxcox(data[column])[0], ax=axes[2],\n                     color='m')\n        axes[2].set_title('Distribution after box-cox')\n    except:\n        print('Not all the data is positive => no box-cox transformation is possible')\n        axes[2].remove()\n        \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def barplot(data, column: str):\n    \"\"\"\n    Plots interactive barplot\n    for given categorical feature\n    \"\"\"\n    trace_0 = go.Bar(x=data[column].value_counts().index,\n                     y=data[column].value_counts().values,\n                     marker_color='violet')\n\n    traces = [trace_0]\n    layout = {'title': 'Churn bar plot',\n              'xaxis': {'title': 'churn category'},\n              'yaxis': {'title': 'number of observations'}}\n\n    fig = go.Figure(data=traces, layout=layout)\n    fig.update_layout(height=600, width=800)\n    iplot(fig, show_link=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numeric features\n\nConducting EDA for numeric features"},{"metadata":{},"cell_type":"markdown","source":"### ClientPeriod"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_unique(df, 'ClientPeriod')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot(df, 'ClientPeriod')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distplot(df, 'ClientPeriod')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"According to \"**ClientPeriod**\" boxplot:\n\n* There are some outliers in churn group 1\n\n\n* No normal distribution can be observed"},{"metadata":{},"cell_type":"markdown","source":"Let's check if the distribution becomes log-normal as we use logarithms:"},{"metadata":{"trusted":true},"cell_type":"code","source":"norm_dist(df, 'ClientPeriod')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The distribution is still not normal but at least a little closer to it\n\n* We'll use log-normal distribution for \"**ClientPeriod**\" later"},{"metadata":{},"cell_type":"markdown","source":"### MonthlySpending"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_unique(df, 'MonthlySpending')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot(df, 'MonthlySpending')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distplot(df, 'MonthlySpending')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We have no outliers according to IQR\n\n* No normal distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting different normal dist transformations\nnorm_dist(df, 'MonthlySpending')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* After logarithmic transformation we got something that looks similar to bimodal distribution but not to normal one\n\n* Box-cox transformation isn't really helpful in this case as well\n\n* We'll also try using log-normal distribution for this feature later"},{"metadata":{},"cell_type":"markdown","source":"## TotalSpent"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_unique(df, 'TotalSpent')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"boxplot(df, 'TotalSpent')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"distplot(df, 'TotalSpent')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* According to IQR the column doesn't contain any outliers\n\n* The distribution of this feature is extremely left-skewed\n\n* So in general most people spend up to 4000 (conventional units) but there are many of those who spend much more"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting different normal dist transformations\nnorm_dist(df, 'TotalSpent')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* So after box-cox trasformation the distribution of **TotalSpent** became much closer to a normal one\n\n\n* We will try to use this one for ML model and check if it performs better"},{"metadata":{},"cell_type":"markdown","source":"As we remember, this feature contained nan-values which were replaced by zeros. Let's take a closer look to these observations:"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.query('TotalSpent == \" \" and ClientPeriod == 0')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"zeros = train.query('ClientPeriod == 0 and TotalSpent != \" \"').shape[0]\nprint(f'Number of observations with zero ClientPeriod and non-zero TotalSpent: {zeros}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* What is noticable - all clients with missing **TotalSpent** have zero **ClientPeriod** and zero **Churn**\n\n* Probably the first idea of imputing **TotalSpent** with zeros was logically correct"},{"metadata":{},"cell_type":"markdown","source":"### Dealing with outliers:"},{"metadata":{},"cell_type":"markdown","source":"Earlier we revealed that there are outliers in some numeric columns in Churn 1 group"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting rid of outliers using IQR\nfor num_col in num_cols:\n    Q1 = df[df['Churn'] == 1][num_col].quantile(0.25)\n    Q3 = df[df['Churn'] == 1][num_col].quantile(0.75)\n    IQR = Q3 - Q1\n    left_lim = Q1 - 1.5*IQR\n    right_lim = Q3 + 1.5*IQR\n    outliers = np.where((df[df['Churn'] == 1][num_col] > right_lim) |\\\n                        (df[df['Churn'] == 1][num_col] < left_lim))[0]\n    print(f'Number of outliers fo {num_col}: {len(outliers)}')\n    if num_col == 'ClientPeriod':\n        df.drop(outliers, errors='ignore', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Using log-normal distribution\ndf['TotalSpent'] = df['TotalSpent'].apply(lambda x: np.log(x + 1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Categorical features\n\nEDA for categorical features:"},{"metadata":{},"cell_type":"markdown","source":"Some basic functions for categorical features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def pie_bar(data, column):\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    axes[0].pie(data[column].value_counts(),\n                labels=data[column].value_counts().index,\n                autopct='%1.1f%%', shadow=True, startangle=90)\n    axes[0].set_title(f'Pie chart for \"{column}\"')\n    \n    sns.barplot(x=data[column].astype(str).value_counts().index,\n                y=data[column].value_counts().values,\n                palette='Spectral', ax=axes[1])\n    axes[1].set_title(f'Bar plot for \"{column}\"')\n    plt.xticks(rotation=90)\n    plt.subplots_adjust(wspace=0.6);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Sex"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_unique(df, 'Sex')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pie_bar(df, 'Sex')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We have almost an equal number of male and female clients"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Differences between spendings among male and female clients\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\naxes[0].set_title('Monthly spending by sex', size=16)\naxes[1].set_title('Total spending by sex', size=16)\nsns.boxplot(df['MonthlySpending'], y=df['Sex'], ax=axes[0], palette=\"Set3\")\nsns.boxplot(df['TotalSpent'], y=df['Sex'], ax=axes[1], palette=\"magma\")\nplt.subplots_adjust(wspace=0.6);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Visually there are no significant differences between spendings of men and women"},{"metadata":{},"cell_type":"markdown","source":"It seems also quite important to check if **Sex** affects the chance that a client will go away:"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['Sex'],\n              hue=df['Churn'],\n              palette='Blues')\nplt.title('Churn by sex');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Well, here we also have no significant difference\n\n* We'll decide later whether to include this feature in ML model or not"},{"metadata":{},"cell_type":"markdown","source":"## IsSeniorCitizen"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_unique(df, 'IsSeniorCitizen')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pie_bar(df, 'IsSeniorCitizen')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Most clients are not seniors"},{"metadata":{},"cell_type":"markdown","source":"## HasPartner"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_unique(df, 'HasPartner')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pie_bar(df, 'HasPartner')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Almost equal amounts of people have and don't have a partner (i. e. a girlfriend/wife, a boyfriend/husband)"},{"metadata":{},"cell_type":"markdown","source":"## HasChild"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_unique(df, 'HasChild')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pie_bar(df, 'HasChild')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* About twice more people have no children"},{"metadata":{},"cell_type":"markdown","source":"## HasPhoneService"},{"metadata":{"trusted":true},"cell_type":"code","source":"show_unique(df, 'HasPhoneService')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pie_bar(df, 'HasPhoneService')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* The majority of people have phone service - quite an obvious result"},{"metadata":{},"cell_type":"markdown","source":"**Too much time spent on plotting each cat-feature one by one.**\n\nLet's go over other features in a cycle:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for cat_col in cat_cols[5:]:\n    show_unique(df, cat_col)\n    pie_bar(df, cat_col)\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Nothing extra-ordinary was noticed for other categorical features"},{"metadata":{},"cell_type":"markdown","source":"# Correlation analysis\n\nConducting correlation analysis of dataset will help us to check if there is a **multicollinearity** problem and to understand better **which features are correlated with our target variable**:"},{"metadata":{},"cell_type":"markdown","source":"## Numeric features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatter plots and dists\nsns.pairplot(df[num_cols + [target_col]], hue=target_col);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* TotalSpent and ClientPeriod are quite dependent\n\n* In general, the more a person pays monthly and the less his/her client period is the more is the chance that he/she will stop using telecom's services"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking differences on boxplots\nfig, axes = plt.subplots(1, 3, figsize=(20, 6))\nfor i, num_col in enumerate(num_cols, start=0):\n    sns.boxplot(y=num_col, x=\"Churn\", data=df, ax=axes[i], palette='husl')\n    axes[i].set_title(f'Boxplots for {num_col} by Churn groups')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Visually we can see clear differences between groups"},{"metadata":{},"cell_type":"markdown","source":"There are several statistical ways to check the dependency between categorical and numeric feature but we'll use **non-parametric Mann-Whitney test** (since the assumption of normal distribution is not met) to check the differences in group means:"},{"metadata":{"trusted":true},"cell_type":"code","source":"for num_col in num_cols:\n    sample_1, sample_2 = df[df['Churn'] == 1][num_col], df[df['Churn'] == 0][num_col]\n    if sample_1.shape[0] > 20 and sample_2.shape[0] > 20:\n        mwu = ss.mannwhitneyu(sample_1, sample_2)\n    else:\n        raise Exception ('At least one of the samples is too small statistically')\n    p_value = mwu.pvalue\n    if p_value < 0.01:\n        print(f'With 99% of confidence significant differences were found for \"{num_col}\" Churn groups')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* So all the numeric features are important, we'll include them in ML-model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation matrix\nplt.figure(figsize=(12, 6))\nsns.heatmap(df[num_cols + [target_col]].corr(), cmap='Blues', annot=True, fmt='.3f', linewidth=0.6)\nplt.title('Correlation matrix for numeric features and target');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Two columns (**TotalSpent** and **ClientPeriod**) correlate a lot -> it's close to **multicollinearity** (corr_coef > 0.8)\n\n* It seems quite natural that the bigger a client's period is, the more this client has already spent\n\n* It is reasonable to conclude that all the numeric features explain our target variable well enough"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # # Deleting MonthlySpending to avoid multicollinearity\n# df.drop(['TotalSpent'], axis=1, inplace=True)\n# num_cols.remove('TotalSpent')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical features"},{"metadata":{},"cell_type":"markdown","source":"To check dependencies between categorical features and target, we'll use **Chi2-test**:"},{"metadata":{"trusted":true},"cell_type":"code","source":"stats = []\nimp_stats = []\nimp_cat_cols = []\nfor cat_col in cat_cols:\n    contingency_table = pd.crosstab(df[cat_col], df['Churn'])\n    chi2_stat, p_value = ss.chi2_contingency(observed=contingency_table)[:2]\n    stats.append(chi2_stat)\n    if p_value < 0.01:\n        imp_stats.append(chi2_stat)\n        imp_cat_cols.append(cat_col)\n        print(f'With 99% of confidence significant differences found for {cat_col}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Number of significant categorical features: {len(imp_stats)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Significant categorical columns:\\n\\n', np.array(imp_cat_cols))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking chi2 statistics\nplt.figure(figsize=(12, 6))\nseries = pd.Series(stats, index=cat_cols).sort_values(ascending=True)\nseries.plot(kind='barh')\nplt.title('Chi2 statistics for categorical features');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Two columns were considered insignificant: **Sex**, **HasPhoneService**\n\n* So my previous assumption on that **Sex** plays no role in defining **Churn** is now proved statistically"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping insignificant categorical columns\ndf.drop(['Sex', 'HasPhoneService'], axis=1, inplace=True)\n\n# Removing them from cat_cols list\ncat_cols.remove('Sex')\ncat_cols.remove('HasPhoneService')\nfeature_cols = num_cols + cat_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"### Polynomial features\n\n* Probably there is a more complicated linear dependency between features and target\n\n* Polynomials (interactions between numeric features) might help to predict target better in this cas"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Trying out polynomial features and interactions of features\n# poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n# poly.fit(df[num_cols])\n# columns = poly.get_feature_names(num_cols)\n# df_poly = pd.DataFrame(poly.transform(df[num_cols]),\n#                        columns=columns)\n# df = pd.concat([df, df_poly], axis=1)\n# num_cols.extend(df_poly.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Nothing positive came out of this experiment \n\n> Polynomials are useless in this case - lead to overfitting"},{"metadata":{},"cell_type":"markdown","source":"# Machine Learning\n\nNow let's train a basic ML-model and then try to improve it by feature engineering and other interesting stuff"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Preprocessing for ML:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X and y in DF form\nX_df = df[feature_cols]\ny = df[target_col]\n\n# Got dummies \ndummies = pd.get_dummies(df[cat_cols], drop_first=True)\n\n# Concatenating with numeric features\nX = pd.concat([df[num_cols], dummies], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting data\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=123)\n\n# Scaling numeric train data\nscaler = RobustScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sample sizes\nX.shape, y.shape, X_train.shape, y_train.shape, X_val.shape, y_val.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Logistic Regression:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic Regression model\nlr_cv = LogisticRegressionCV(Cs=np.linspace(1, 10, 10),\n                             cv=5, scoring='roc_auc',\n                             solver='lbfgs', max_iter=500,\n                             refit=True)\nlr_cv.fit(X_train_scaled, y_train)\n\n# Predicting\ny_pred = lr_cv.predict(X_val_scaled)\ny_pred_proba = lr_cv.predict_proba(X_val_scaled)[:, 1]\ny_train_pred_proba = lr_cv.predict_proba(X_train_scaled)[:, 1]\n\nprint(f'ROC-AUC LR CV: {roc_auc_score(y_val, y_pred_proba)}\\nF1-Score LR CV: {f1_score(y_val, y_pred)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting roc-auc curve\ntrain_auc = roc_auc_score(y_train, y_train_pred_proba)\nval_auc = roc_auc_score(y_val, y_pred_proba)\n\nplt.figure(figsize=(10,5))\nplt.plot(*roc_curve(y_train, y_train_pred_proba)[:2], label='train AUC={:.4f}'.format(train_auc))\nplt.plot(*roc_curve(y_val, y_pred_proba)[:2], label='test AUC={:.4f}'.format(val_auc))\nlegend_box = plt.legend(fontsize='large', framealpha=1).get_frame()\nlegend_box.set_facecolor(\"white\")\nlegend_box.set_edgecolor(\"black\")\nplt.plot(np.linspace(0,1,100), np.linspace(0,1,100))\nplt.title('ROC-AUC for train and validation set')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Pipeline"},{"metadata":{},"cell_type":"markdown","source":"### Pipeline structure:\n\n1) Use custom transformer to add new features\n\n1) Use **ColumnTransformer**: to drop the columns that are not required for model training, to scale numeric features using *RobustScaler()*\n\n\n2) Use ML model to predict **Churn**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Step 1 - Preprocess TotalSpent - use for input data\n# class ColumnFormatter(BaseEstimator):\n#     \"\"\"\n#     Changes TotalSpent column values\n#     to logarithmic values\n#     \"\"\"\n#     def __init__(self):\n#         pass\n    \n#     def fit(self, X, y=None):\n#         return self\n    \n#     def transform(self, X):\n#         # Formatting TotalSpent\n#         X['TotalSpent'] = X['TotalSpent'].replace(' ', '0')\n#         X['TotalSpent'] = X['TotalSpent'].astype('float64')\n#         X['TotalSpent'] = X['TotalSpent'].apply(lambda x: np.log(x + 1))\n        \n#         return X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Step 2 - preprocess the other data\npreprocess = ColumnTransformer(remainder='passthrough',\n                               transformers=[\n                                            \n                                             ('binary_data', OneHotEncoder(drop='first'), ['IsSeniorCitizen',\n                                                                                           'HasMultiplePhoneNumbers',\n                                                                                           'HasPartner',\n                                                                                           'HasChild',\n                                                                                           'HasInternetService',\n                                                                                           'HasOnlineSecurityService',\n                                                                                           'HasOnlineBackup',\n                                                                                           'HasDeviceProtection',\n                                                                                           'HasTechSupportAccess',\n                                                                                           'HasOnlineTV',\n                                                                                           'HasMovieSubscription',\n                                                                                           'HasContractPhone',\n                                                                                           'IsBillingPaperless',\n                                                                                           'PaymentMethod']),\n                                            \n                                             ('scale_data', RobustScaler(), ['ClientPeriod',\n                                                                            'MonthlySpending',\n                                                                            'TotalSpent'])\n                                            ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a pipeline\npip_log = Pipeline(steps=[\n                          ('pre_processing', preprocess),\n                          ('logistic_regression', LogisticRegression(max_iter=1000, \n                                                                     random_state=123))\n                         ]\n                  )\n\n# Splitting the original data\nX_train, X_val, y_train, y_val = train_test_split(X_df, y,\n                                                  test_size=0.2,\n                                                  random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Grid search for pipeline model\nparams = {'logistic_regression__C': np.linspace(0, 10, 10),\n          'logistic_regression__class_weight': ['none', 'balanced'],\n          'logistic_regression__solver': ['newton-cg', 'lbfgs', 'sag']}\n\ngrid = GridSearchCV(pip_log, params, cv=5, n_jobs=-1, scoring='roc_auc', verbose=1)\ngrid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Best score for pipeline: {grid.best_score_}')\nprint(f'Best pipeline: {grid.best_estimator_}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plotting roc-auc curve\ngrid_train_pred_proba = grid.predict_proba(X_train)[:, 1]\ngrid_val_pred_proba = grid.predict_proba(X_val)[:, 1]\n\ntrain_auc = roc_auc_score(y_train, grid_train_pred_proba)\nval_auc = roc_auc_score(y_val, grid_val_pred_proba)\n\nplt.figure(figsize=(10,5))\nplt.plot(*roc_curve(y_train, grid_train_pred_proba)[:2], label='train AUC={:.4f}'.format(train_auc))\nplt.plot(*roc_curve(y_val, grid_val_pred_proba)[:2], label='test AUC={:.4f}'.format(val_auc))\nlegend_box = plt.legend(fontsize='large', framealpha=1).get_frame()\nlegend_box.set_facecolor(\"white\")\nlegend_box.set_edgecolor(\"black\")\nplt.plot(np.linspace(0,1,100), np.linspace(0,1,100))\nplt.title('ROC-AUC for train and validation set')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Seems like this model predicts a little better on test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining best logistic regression model\nlog_reg_best = grid.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gradient boosting"},{"metadata":{},"cell_type":"markdown","source":"We'll use Yandex CatBoost library which is quite convenient to work with categoricals and gives accurate results with no hypeparameter tuning."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data with no dummies\nX_train, X_val, y_train, y_val = train_test_split(X_df, y, test_size=0.2,\n                                                    random_state=123)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Training catboost classifier model\ncbc = CatBoostClassifier(custom_loss='AUC', cat_features=cat_cols,\n                         verbose=False)\ncbc.fit(X_train, y_train, eval_set=(X_val, y_val), plot=True)\ncbc_val_pred_proba = cbc.predict_proba(X_val)[:, 1]\ncbc_train_pred_proba = cbc.predict_proba(X_train)[:, 1]\n\n# Checking roc-auc metrics\nprint(f'Validation ROC-AUC: {roc_auc_score(y_val, cbc_val_pred_proba)}')\nprint(f'Training ROC-AUC: {roc_auc_score(y_train, cbc_train_pred_proba)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # catboost grid search cv\n# cbc_grid = CatBoostClassifier(n_estimators=500, silent=True,\n#                               cat_features=cat_cols, eval_metric='AUC')\n\n# cbc_grid.grid_search({'l2_leaf_reg': np.linspace(0, 10, 20),\n#                       'depth': np.arange(2, 10, 2)},\n#                      X_df, y, cv=5,\n#                      plot=True, refit=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Best parameters\n# best_params = cbc_grid.get_params()\n\nparams = {'silent': True,\n          'eval_metric': 'AUC',\n          'n_estimators': 500,\n          'cat_features': ['IsSeniorCitizen',\n                          'HasPartner',\n                          'HasChild',\n                          'HasMultiplePhoneNumbers',\n                          'HasInternetService',\n                          'HasOnlineSecurityService',\n                          'HasOnlineBackup',\n                          'HasDeviceProtection',\n                          'HasTechSupportAccess',\n                          'HasOnlineTV',\n                          'HasMovieSubscription',\n                          'HasContractPhone',\n                          'IsBillingPaperless',\n                          'PaymentMethod'],\n          'depth': 4,\n          'l2_leaf_reg': 2}\n\n\n# Best model\ncbc_best = CatBoostClassifier(**params)\ncbc_best.fit(X_train, y_train, eval_set=(X_val, y_val), plot=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking the metric on validation set\nbest_metric = roc_auc_score(y_val, cbc_best.predict_proba(X_val)[:, 1])\nprint(f\"Best gradient boosting score: {best_metric}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preprocessing testing set\ntest['TotalSpent'] = test['TotalSpent'].replace(' ', '0').astype('float64')\ntest['TotalSpent'] = test['TotalSpent'].apply(lambda x: np.log(x + 1))\ntest['IsSeniorCitizen'] = test['IsSeniorCitizen'].astype('category')\ntest.drop(['Sex', 'HasPhoneService'], axis=1, inplace=True)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Final predictions\nsub['Churn'] = cbc_best.predict_proba(test)[:, 1]\nsub.to_csv('submission.csv', index=False)\ndisplay(sub)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}